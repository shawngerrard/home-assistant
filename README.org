#+title: Home Assistant Setup Instructions

This will take us through the steps of setting up Home Assistant on Ubuntu Server.


** Prerequisites

To use these instructions, you'll need Ubuntu Server installed on a machine, as well as SSH access (OpenSSH) to the server.

You'll need to install the following tools to use this repository:

#+begin_quote
*Note*: If using my [[https://github.com/shawngerrard/ubuntu-tooling/][ubuntu-tool setup]], these tools should already be installed/configured.
#+end_quote

- [[https://www.gnu.org/software/gawk/][GAWK]]
- [[https://nmap.org/][NMAP]]
- [[https://www.pulumi.com/docs/install/][Pulumi CLI]]
- [[https://cloud.google.com/sdk/docs/install][GCP CLI]]
- [[https://github.com/nvm-sh/nvm?tab=readme-ov-file#installing-and-updating][NVM]]
- [[https://www.npmjs.com/][NPM]] and [[https://nodejs.org/en][Node]]
- [[https://www.typescriptlang.org/][Typescript]]
- [[https://www.npmjs.com/package/ts-node][TS-Node]]

** Specification

I'm running Home Assistant on the following specs:
  - Raspberry Pi model 4b
  - Ubuntu Server 64-bit LTS 22.04
  - 3TB SSD mounted storage volume

*** Future updates:

***** TODO Administer infra stack with Pulumi configuration
***** TODO Change from single node infra to multi-node for high-availability.
***** TODO Replace Raspberry Pi with more powerful server hardware.
***** TODO Replace Ubuntu Server with RedHat Enterprise Linux (RHEL).

** Pulumi authentication

If you haven't already, we must authenticate with Pulumi in order to interact with the Pulumi Cloud API. We can do this a number of different ways - either log in via CLI or use an access token.

#+begin_quote
*Note*: Please refer to the definitions of both organizational and personal access tokens on the [[https://www.pulumi.com/docs/pulumi-cloud/access-management/access-tokens/][Pulumi website]]. This link will also provide the information on how to generate an access token. For the purposes of this guide, we will be creating a _personal access token_.
#+end_quote

#+begin_quote
*Note*: Neither of the options listed below are secure on their own - please refer to Pulumi regarding [[https://www.pulumi.com/blog/using-pulumi-securely/][how to use it securely]].
#+end_quote

*** Update credentials.json with access token

We can update the ~â€‹~/.pulumi/credentials.json~ with the appropriate access token:

#+begin_quote
{
    "current": "https://api.pulumi.com",
    "accessTokens": {
        "https://api.pulumi.com": "<access token>"
    },
    "accounts": {
        "https://api.pulumi.com": {
            "accessToken": "<access token>",
            "username": "<username>",
            "organizations": [
                "<org1>",
                "<org2>"
            ],
            "lastValidatedAt": "2024-04-16T11:35:59.084951264+12:00"
        }
    }
}
#+end_quote

*** Update an environment variable

We could also use the ~PULUMI_ACCESS_TOKEN~ environment variable to store our access token.

** Google Cloud authentication

If you haven't already, we must authenticate with Google in order to interact with the Google Cloud API by setting a project and using a link supplied by Google to optain an access code.

#+begin_src bash
# Set project ID in config
gcloud config set project <YOUR_GCP_PROJECT_ID>

# Obtain a url from Google to log in
gcloud auth login --no-launch-browser
#+end_src

Use the code from the URL provided to obtain an access code and enter the code into the CLI.

** Pulumi backend configuration

Make sure to update the Pulumi configuration to use the correct organization by default:

#+begin_src bash
pulumi org set-default <org-name>
#+end_src

I've named the default organization for these projects as ~myhome~.

** Infrastructure deployment (k3s/helm)

This section will deploy ~k3s~ infrastructure to a ~raspberry-pi~ using Pulumi.

#+begin_quote
*Note*: Make sure you run all commands in this section in the correct sub-folder. I.E - ~cd [path/to]/home-assistant/infra-k3s~
#+end_quote

*** Scaffold and configuration

We must first create and configure the ~typescript~ pulumi project that will be used to install our infrastructure applications (E.G - ~k3s~, ~helm~, etc).

**** New project

#+begin_quote
*Note*: If using my github code repository for these deployments, you can ignore this section and follow the next section _Existing project_.
#+end_quote

First, we need to create a stack to manage our infrastructure.

We can create and configure our stack by passing in the ~--config~ parameter to the ~pulumi new~ command, as follows:

#+begin_src bash
# Create the pulumi project and pass in configuration key/value pairs
pulumi new typescript --name "infra-k3s" \
    --stack "dev" \
    --secret \
    --config="serverKey=$(cat /path/to/private/ssh/key)" \
    --config="serverIp=$(nmap -n -A <server hostname> -oG - | awk '/Up$/{print $2}')" \
    --config="serverUser=<server username>"
#+end_src

#+begin_quote
*Note*: This will create a new typescript pulumi project named ~infra-k3s~, a stack named ~dev~, and a configuration file (~Pulumi.dev.yaml~) containing the server SSH key for remote connection as well as the IP address (obtained from ~nmap~) of the server and user account logging in.
#+end_quote

**** Existing project

First, we must clone our project from the repository:

#+begin_src bash
git clone git@github.com:shawngerrard/home-assistant.git ~/documents/
#+end_src

Then, we must select the ~dev~ stack of our infrastructure project ~infra-k3s~:

#+begin_src bash
cd ~/documents/home-assistant/infra-k3s && pulumi stack select myhome/dev
#+end_src

And also populate the pulumi configuration file for ~infra-k3s~:

#+begin_src bash
cd ~/documents/home-assistant/infra-k3s
cat ~/path/to/ssh/private/key | pulumi config set --secret serverKey
nmap -n -A <server hostname> -oG - | awk '/Up$/{print $2}' | pulumi config set serverIp
pulumi config set serverUser <user name>
#+end_src

Finally, for the projects to compile we must download the code package dependencies from ~npm~ referenced in the project's ~package.json~:

#+begin_src bash
cd ~/documents/home-assistant/infra-k3s && npm install
#+end_src

*** Deployment

With the infrastructure to deploy defined within the project (typically ~index.ts~), we can deploy our ~infra-k3s~ suite.

#+begin_src bash
# Deploy the pulumi infra-k3s dev stack
cd ~/documents/home-assistant/infra-k3s && pulumi up -y
#+end_src

When the stack is up, you should have the ~kubeconfig~ file from the cluster present on your local system and configured to connect to your local server. You can view this with ~cat ~/.kube/config~.

Once the ~infra-k3s~ stack is deployed, we can continue on with the rest of the deployments below.

** Ingress controller deployment (nginx-ingress-controller)

This section will deploy the ~nginx-ingress-controller~ to the ~k3s~ infrastructure defined in the _Infrastructure deployment_ section.

#+begin_quote
*Note*: Make sure you run the commands in this section in the correct sub-folder. I.E - ~cd [path/to]/home-assistant/app-nginx~
#+end_quote

*** Scaffold and configuration

Once the ~infra-k3s~ stack is up, we're ready to deploy our ~app-nginx~ ingress controller.

Using the code repository we cloned from Github in the _Infrastructure deployment_ section, we must select the ~myhome~ org and ~dev~ stack of our application deployment project ~app-nginx~:

#+begin_src bash
cd ~/documents/home-assistant/app-nginx && pulumi stack select myhome/dev
#+end_src

The code attempts to obtain stack references from the ~myhome/infra-k3s/dev~ stack. To enable Pulumi to achieve this, we need to populate the pulumi configuration file for ~app-nginx~:

#+begin_src bash
cd ~/documents/home-assistant/app-nginx
pulumi config set org myhome
pulumi config set serverProject infra-k3s
#+end_src

Finally, for the projects to compile we must download the code package dependencies from ~npm~ referenced in the project's ~package.json~:

#+begin_src bash
cd ~/documents/home-assistant/app-nginx && npm install
#+end_src

*** Deployment

With the applications to deploy defined within the project (typically ~index.ts~), we can deploy our ~app-nginx~ suite.

Now we should be able to raise the Pulumi stack:

#+begin_src bash
# Deploy the pulumi dev stack
cd ~/documents/home-assistant/app-nginx && pulumi up -y
#+end_src

** Application deployment (home-assistant)

This section will deploy the ~home-assistant~ application to the ~k3s~ infrastructure defined in the _Infrastructure deployment_ section.

#+begin_quote
*Note*: Make sure you run the commands in this section in the correct sub-folder. I.E - ~cd [path/to]/home-assistant/app-homeassistant~
#+end_quote

*** Scaffold and configuration

Once the ~infra-k3s~ stack is up, we're ready to deploy our ~app-homeassistant~ application.

Using the code repository we cloned from Github in the _Infrastructure deployment_ section, we must select the ~myhome~ org and ~dev~ stack of our application deployment project ~app-homeassistant~:

#+begin_src bash
cd ~/documents/home-assistant/app-homeassistant && pulumi stack select myhome/dev
#+end_src

The code attempts to obtain stack references from the ~myhome/infra-k3s/dev~ stack. To enable Pulumi to achieve this, we need to populate the pulumi configuration file for ~app-homeassistant~:

#+begin_src bash
cd ~/documents/home-assistant/app-homeassistant
cat ~/path/to/ssh/private/key | pulumi config set --secret serverKey
pulumi config set org myhome
pulumi config set serverProject infra-k3s
#+end_src

For the project to compile we must download the code package dependencies from ~npm~ referenced in the project's ~package.json~:

#+begin_src bash
cd ~/documents/home-assistant/app-homeassistant && npm install
#+end_src

Ideally, we want to use and configure a dedicated DNS server to resolve the hostname ~dev.homeassistant.local~. However, we can get around this by modifying our local ~hosts~ file.

#+begin_quote
*Note*: The ~hosts~ file can be found:
 - In UNIX: ~/etc/hosts~
 - In Windows: ~C:\Windows\System32\drivers\etc\hosts~
#+end_quote

At the end of the file, we can place the following:

#+begin_src
<local ip address of server> dev.homeassistant.local
#+end_src

*** Deployment

With the applications to deploy defined within the project (typically ~index.ts~), we can deploy our ~app-homeassistant~ suite.

We must ensure that we've authenticated with the Google API:

#+begin_src
gcloud auth application-default login
#+end_src

Now we should be able to raise the Pulumi stack:

#+begin_src bash
# Deploy the pulumi dev stack
cd ~/documents/home-assistant/app-homeassistant && pulumi up -y
#+end_src

Once the stack is up, we need to configure ~home-assistant~ so that it will trust ~nginx-ingress-controller~ as a reverse proxy, otherwise the ~home-assistant~ logs will show errors when attempting to make connections using the hostname.

#+begin_quote
*Note*: We could use a ConfigMap to perform the following steps, however this _may_ make data volume snapshots more difficult to achieve. There's an [[https://github.com/pajikos/home-assistant-helm-chart/issues/30][open issue on Github]] relating to this and our next steps.
#+end_quote

To configure ~home-assistant~, we must first access the pod:

#+begin_src bash
kubectl exec -it $(kubectl get pod -l app.kubernetes.io/name=home-assistant -n home-assistant-dev -o jsonpath="{.items[0].metadata.name}") -n home-assistant-dev -- bash
#+end_src

Then, we need to edit the ~configuration.yaml~ by using the ~vi /config/configuration.yaml~ command. We then add the following at the bottom of the file:

#+begin_src
http:
  use_x_forwarded_for: true
  trusted_proxies:
    - 10.42.0.0/16 # or we can use the IP to the nginx pod - 10.42.0.n
#+end_src

Save by pressing ~ESC~ and type ~:wq~ to save and exit the file. Try accessing ~dev.homeassistant.local~, however we may need to refresh the ~home-assistant~ pod with:

#+begin_src
kubectl rollout restart -n home-assistant statefulset.apps/home-assistant
#+end_src

** Pod destruction and restructure errors

It's important to note of what happens on the occassion that the ~home-assistant~ pod crashes or is destructed, in terms of the effect this will have on the kubernetes resources and stack state.

Effectively, when the ~home-assistant~ pod is taken down either through an error, a ~kubectl delete pod <pod name>~ command, or the ~home-assistant~ pulumi stack is destroyed with ~pulumi destroy~, we would attempt to bring the pod back online somehow - either through applying a direct redeployment of the statefulset, by initiating the ~pulumi up~ command to update the stack, or automatically by the control plane depending on the deployment topology and specifications (I.E high-availability/redundancy setups).

Because ~pulumi~ doesn't have a methods to intercept events when resources created from a ~Helm chart~ are destroyed, there's no way native to ~pulumi~ to push commands to the kubernetes api server when these resources are destroyed. This means that the attached PersistentVolume definition remains in a ~released~ state as the ~uid~ is bound to the recently destroyed PersistentVolumeClaim (observable with ~kubectl get pv -o yaml~ and comparing to the ~uid~ in ~kubectl get pvc -o yaml~).

This in-turn means that the pvc created by the ~home-assistant~ chart cannot be bound to the existing PV without flushing the ~uid~ of the pv with:

#+begin_src bash
kubectl patch pv <pv name> --type json -p '[{"op": "remove", "path": "/spec/claimRef/uid"}]'
#+end_src

There's multiple ways to deal with this:

 - Set the reclaim policy of the pv's to ~delete~ rather than ~retain~ - this may result in data loss. I don't recommend this option without a data recovery/backup procedure (not yet covered by this repository).

 - Manually run either of the commands when the pvc is deleted:

   #+begin_src bash
# Run this command to remove the UID field from the pv spec
kubectl patch pv packages-volume --type json -p '[{"op": "remove", "path": "/spec/claimRef/uid"}]'

# Run this command to remove the whole claimRef section from the pv spec
kubectl patch pv packages-volume --type json -p '[{"op": "remove", "path": "/spec/claimRef"}]'
   #+end_src

 - Define a kubernetes ~job~ in ~pulumi~ to poll the api server for instances of when a pvc is being destroyed, and then initiate a function to remove the uid from the pv spec:

   #+begin_src typescript
import * as pulumi from "@pulumi/pulumi";
import * as k8s from "@pulumi/kubernetes";

// Define the PVC deletion job
const pvcDeletionJob = new k8s.batch.v1.Job("pvc-deletion-job", {
    spec: {
        template: {
            metadata: {
                labels: {
                    app: "pvc-deletion-handler",
                },
            },
            spec: {
                containers: [{
                    image: "bitnami/kubectl",
                    args: [
                        "wait",
                        "--for=delete",
                        "pvc",
                        "--selector=app=my-helm-chart",
                    ],
                    name: "pvc-deletion-watcher",
                }],
                restartPolicy: "Never",
            },
        },
    },
});

// Define the cleanup operation to remove the UID link from the PV metadata
const removeUIDLink = async () => {
    console.log("Removing UID link from PV metadata");
    // Implement the logic to remove the UID link from the PV metadata
};

// Run the cleanup operation when the PVC deletion job completes
pvcDeletionJob.status.apply(status => {
    if (status && status.succeeded) {
        removeUIDLink();
    }
});

// Export any outputs if needed
export const jobName = pvcDeletionJob.metadata.name;
   #+end_src

 - Create a [[https://slack.engineering/simple-kubernetes-webhook/][kubernetes webhook server]] that uses the [[https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/][kubernetes admission webhooks]] to intercept api requests to the kubernetes api and then mutate responses so that the associated pv's bound uid is flushed upon deletion of a pvc.

 - Create a [[https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/ha-topology/][kubernetes highly-available topology]] to minimize application downtime and improve fault tolerance, and then utilize any of the previous steps to manage the failed pod restart.

The more suitable method would be to either use the admission webhooks or opt for a highly-available topology. Because this project is prioritizing learning different aspects of the cloud-native realms, I'm opting for a simpler manual approach for now.

#+begin_quote
*TL;DR*: Whenever the ~home-assistant~ pod or ~app-homeassistant~ pulumi stack is destroyed, we need to run the ~kubectl patch pv packages-volume --type json -p '[{"op": "remove", "path": "/spec/claimRef/uid"}]'~ command on the server before restarting the app stack/pod.
#+end_quote

*** TODO Implement either a webhook server or highly available topology to manage pvc reclaims

** Integrating Google Cloud with Home Assistant

This guide will outline how integrate ~home-assistant~ within a ~gcp~ project by using the [[https://console.cloud.google.com/][Google Cloud console]] to configure an OAuth 2.0 client, and then use the client to facilitate integration between ~home-assistant~ and a new [[https://console.nest.google.com/][Google Nest project]].

#+begin_quote
*Note*: These steps are mostly manual as Google does not offer an API to programatically create or update OAuth 2.0 Clients - refer to [[https://stackoverflow.com/questions/69539734/pulumi-gcp-manage-oauth2-0-client-id-redirect-uri][StackOverflow and the link in the answer section]] for more information. These manual steps can also be followed by the guide provided in Home Assistant when adding a new device.
#+end_quote

#+begin_quote
*Note*: Refer to the section _Google Cloud authentication_ above to set the ~gcp~ configuration and log in to the correct project.
#+end_quote

*** Future updates

**** TODO Investigate use of gcp identity platform (idp) to automate creation of OAuth 2.0 Clients in GCP.

- [[https://cloud.google.com/identity-platform/pricing#identity-platform-pricing][IDP appears mostly free]] - refer to the Google pricing model.

**** TODO Investigate use of service accounts within Home Assistant and GCP to automate deployment.

*** Enable GCP services

First, we must enable the required integration services within ~gcp~:

#+begin_src bash
# Enable the smart device management api in the gcp api library
gcloud services enable smartdevicemanagement.googleapis.com

# Enable the cloud pub/sub api in the gcp api library
gcloud services enable pubsub.googleapis.com

# Enable the iam api in the gcp api library
gcloud services enable iam.googleapis.com
#+end_src

*** Create an Oauth consent for GCP

Next, we must consent for GCP to be available to external users.

This must be enabled in the ~gcp~ console. Access _API's & Services_ in the left-menu of the ~gcp~ dashboard, select _OAuth consent screen_ in the left-menu of the next page, and then select "External" and click the _Create_ button.

Fill out the relevant information:

#+begin_quote
*App name*: home-assistant
*User support email*: <Your-Email>
#+end_quote

*** Create credentials for a new Oauth Client ID

Under _Credentials_, select _Create Credentials_ and the _Oauth client ID_.

Fill out the relevant information:

#+begin_quote
*Application type*: Web application
*Name*: home-assistant-oauth-client
#+end_quote

Under _Authorized redirect URIs_, select *Add URI* as enter ~https://my.home-assistant.io/redirect/oauth~. Then, click the _Create_ button.

Note the *Client ID* and *Client Secret* - you need to add these into ~home-assistant~ when adding a new Google device.

*** Create a device access project

Next, access the [[https://console.nest.google.com/device-access/][Nest device access console]] and follow through the payment flow - this will cost a one-off $5 developer registration fee.

Once you have access to the console, create a new project. You'll need to add in the *Client ID* you created earlier.

Note the *Project ID* - you need to add this into ~home-assistant~ when adding a new Google device.

*** Create a Home Assistant administrator

You can access ~home-assistant~ by pointing your browser to ~http://<your server local ip address>:<nodeport port>~.

Once there, create a new user to enter the ~home-assistant~ dashboard.

*** Add a Google Nest device to Home Assistant

In ~home-assistant~, go to *Settings > Devices and services* and then click _Add device_.

Follow the steps and enter in the details provided from the steps above (gcp project id, oauth client id, nest device access project id).

** Install Home Assistant with Docker

To start with, we'll be running our Home Assistant application in a Docker container.

To install Docker, consult the official Docker [[https://docs.docker.com/engine/install/ubuntu/][installation instructions]] for Ubuntu.

For reference: I've installed Docker using their ~apt~ repository.

#+begin_quote
This Docker installation requires root privileges to run Docker containers. To run Docker containers as a non-root user, follow the official Docker [[https://docs.docker.com/engine/install/linux-postinstall/][Linux post-installation instructions]].
#+end_quote

#+begin_quote
Any OCI (Open Container Initiative) compatible runtime can be used to run a Home Assistant container.
#+end_quote

*** Future updates:

***** TODO Replace Docker with a single-node Kubernetes cluster.
***** TODO Install RedHat OpenShift (RHOS) over the top of Kubernetes to better orchestrate the platform.

** Create a Home Assistant Docker container

With Docker installed, we can start Home Assistant in an OCI (Open Container Initiatie) container.

#+begin_src sh :shebang "#!/bin/bash" :notangle
docker run -d \
  --name homeassistant \
  --privileged \
  --restart=unless-stopped \
  -e TZ=MY_TIME_ZONE \
  -v /PATH_TO_YOUR_CONFIG:/config \
  -v /run/dbus:/run/dbus:ro \
  --network=host \
  ghcr.io/home-assistant/home-assistant:stable
#+end_src

#+begin_quote
- /PATH_TO_YOUR_CONFIG points at the folder where you want to store your configuration and run it. Make sure that you keep the :/config part.

- MY_TIME_ZONE is a tz database name, like TZ=America/Los_Angeles.

- D-Bus is optional but required if you plan to use the Bluetooth integration.
#+end_quote

#+begin_src sh :shebang "#1/bin/bash" :tangle
docker run -d \
  --name homeassistant \
  --privileged \
  --restart=unless-stopped \
  -e TZ=Pacific/Auckland \
  -v /mnt/data:/config \
  -v /run/dbus:/run/dbus:ro \
  --network=host \
  ghcr.io/home-assistant/home-assistant:stable
#+end_src

** Access the Home Assistant dashboard

Once the Docker container is up and running, the Home Assistant dashboard will be accessible using ~http://<host name/host ip network address>:8123/~.
