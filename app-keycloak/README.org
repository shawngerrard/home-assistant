* Application deployment (keycloak)

This section will deploy the ~keycloak~ application to the ~k3s~ infrastructure defined in the _Infrastructure deployment_ section.

#+begin_quote
*Note*: Make sure you run the commands in this section in the correct sub-folder. I.E - ~cd [path/to]/home-assistant/app-keycloak~
#+end_quote

** Configuration

Once the [[../app-homeassistant/README.org][app-homeassistant]] stack is up, we're ready to deploy our ~app-keycloak~ application.

Using the code repository we cloned from Github in the _Infrastructure deployment_ section, we must select the ~myhome~ org and ~dev~ stack of our application deployment project ~app-keycloak~:

#+begin_src bash
cd ~/documents/home-assistant/app-keycloak && pulumi stack select myhome/dev
#+end_src

The code attempts to obtain stack references from the ~myhome/infra-k3s/dev~ stack. To enable Pulumi to achieve this, we need to populate the pulumi configuration file for ~app-keycloak~:

#+begin_src bash
cd ~/documents/home-assistant/app-homeassistant
pulumi config set org $(pulumi org get-default)
pulumi config set serverProject infra-k3s
pulumi config set certManagerProject app-certmanager
cat ~/path/to/ssh/private/key | pulumi config set --secret serverKey
#+end_src

For the project to compile we must download the code package dependencies from ~npm~ referenced in the project's ~package.json~:

#+begin_src bash
cd ~/documents/home-assistant/app-homeassistant && npm install
#+end_src

Ideally, we want to use and configure a dedicated DNS server to resolve the hostname ~dev.homeassistant.local~. However, we can get around this by modifying our local ~hosts~ file.

#+begin_quote
*Note*: The ~hosts~ file can be found:
 - In UNIX: ~/etc/hosts~
 - In Windows: ~C:\Windows\System32\drivers\etc\hosts~
#+end_quote

At the end of the file, we can place the following:

#+begin_src
<local ip address of server> dev.homeassistant.local
#+end_src

** Deployment

With the applications to deploy defined within the project (typically ~index.ts~), we can deploy our ~app-homeassistant~ suite.

We must ensure that we've authenticated with the Google API:

#+begin_src
gcloud auth application-default login
#+end_src

Now we should be able to raise the Pulumi stack:

#+begin_src bash
# Deploy the pulumi dev stack
cd ~/documents/home-assistant/app-homeassistant && pulumi up -y
#+end_src

#+begin_quote
*Note*: Once the stack is up, we need to configure ~home-assistant~ so that it will trust ~nginx-ingress-controller~ as a reverse proxy, otherwise the ~home-assistant~ logs will show errors when attempting to make connections using the hostname.
#+end_quote

#+begin_quote
*Note*: We could use a ConfigMap to perform the following steps, however this _may_ make data volume snapshots more difficult to achieve. There's an [[https://github.com/pajikos/home-assistant-helm-chart/issues/30][open issue on Github]] relating to this and our next steps. There's a fix PR awaiting merging, so this should be modifiable from the ~values.yaml~ file in the future.

In the meantime, we can update the ~configuration.yaml~ file in the ~home-assistant~ container. We can access the container using the following:

#+begin_src bash
kubectl exec -it -n <home-assistant namespace> <home-assistant pod name> -- bash
#+end_src

Open the ~configuration.yaml~ file using ~vi~:

#+begin_src bash
vi /config/configuration.yaml
#+end_src

Enter the following configuration into the file (as per [[https://www.home-assistant.io/integrations/http/][the home-assistant spec]]) and save by pressing ~ESC :wq ENTER~:

#+begin_src yaml
http:
  use_x_forwarded_for: true
  trusted_proxies:
    - 10.42.0.0/16  # Subnet mask for the reverse proxy
#+end_src

Restart the pod with either of the following commands (option 1 recommended as this reduces pod downtime):

#+begin_src bash
# Option 1 (recommended) - refresh the statefulset with a rolling restart to the home-assistant pod
kubectl rollout restart -n home-assistant-dev statefulset.apps/home-assistant

# Option 2 - force the pod to terminate and restart
kubectl get pod <home-assistant pod name> -n <home-assistant namespace> -o yaml | kubectl replace --force -f -
#+end_src

Watch and wait for the pod to be restarted:

#+begin_src bash
watch -n 2 'kubectl get pods -n <home-assistant namespace>'
#+end_src
#+end_quote

Try accessing ~https://dev.homeassistant.local~ - this local URL should now resolve.

* Pod destruction and restructure errors

It's important to note of what happens on the occassion that the ~home-assistant~ pod crashes or is destructed, in terms of the effect this will have on the kubernetes resources and stack state.

Effectively, when the ~home-assistant~ pod is taken down either through an error, a ~kubectl delete pod <pod name>~ command, or the ~home-assistant~ pulumi stack is destroyed with ~pulumi destroy~, we would attempt to bring the pod back online somehow - either through applying a direct redeployment of the statefulset, by initiating the ~pulumi up~ command to update the stack, or automatically by the control plane depending on the deployment topology and specifications (I.E high-availability/redundancy setups).

Because ~pulumi~ doesn't have a methods to intercept events when resources created from a ~Helm chart~ are destroyed, there's no way native to ~pulumi~ to push commands to the kubernetes api server when these resources are destroyed. This means that the attached PersistentVolume definition remains in a ~released~ state as the ~uid~ is bound to the recently destroyed PersistentVolumeClaim (observable with ~kubectl get pv -o yaml~ and comparing to the ~uid~ in ~kubectl get pvc -o yaml~).

This in-turn means that the pvc created by the ~home-assistant~ chart cannot be bound to the existing PV without flushing the ~uid~ of the pv with:

#+begin_src bash
kubectl patch pv <pv name> --type json -p '[{"op": "remove", "path": "/spec/claimRef/uid"}]'
#+end_src

There's multiple ways to deal with this:

 - Set the reclaim policy of the pv's to ~delete~ rather than ~retain~ - this may result in data loss. I don't recommend this option without a data recovery/backup procedure (not yet covered by this repository).

 - Manually run either of the commands when the pvc is deleted:

   #+begin_src bash
# Run this command to remove the UID field from the pv spec
kubectl patch pv packages-volume --type json -p '[{"op": "remove", "path": "/spec/claimRef/uid"}]'

# Run this command to remove the whole claimRef section from the pv spec
kubectl patch pv packages-volume --type json -p '[{"op": "remove", "path": "/spec/claimRef"}]'
   #+end_src

 - Define a kubernetes ~job~ in ~pulumi~ to poll the api server for instances of when a pvc is being destroyed, and then initiate a function to remove the uid from the pv spec:

   #+begin_src typescript
import * as pulumi from "@pulumi/pulumi";
import * as k8s from "@pulumi/kubernetes";

// Define the PVC deletion job
const pvcDeletionJob = new k8s.batch.v1.Job("pvc-deletion-job", {
    spec: {
        template: {
            metadata: {
                labels: {
                    app: "pvc-deletion-handler",
                },
            },
            spec: {
                containers: [{
                    image: "bitnami/kubectl",
                    args: [
                        "wait",
                        "--for=delete",
                        "pvc",
                        "--selector=app=my-helm-chart",
                    ],
                    name: "pvc-deletion-watcher",
                }],
                restartPolicy: "Never",
            },
        },
    },
});

// Define the cleanup operation to remove the UID link from the PV metadata
const removeUIDLink = async () => {
    console.log("Removing UID link from PV metadata");
    // Implement the logic to remove the UID link from the PV metadata
};

// Run the cleanup operation when the PVC deletion job completes
pvcDeletionJob.status.apply(status => {
    if (status && status.succeeded) {
        removeUIDLink();
    }
});

// Export any outputs if needed
export const jobName = pvcDeletionJob.metadata.name;
   #+end_src

 - Create a [[https://slack.engineering/simple-kubernetes-webhook/][kubernetes webhook server]] that uses the [[https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/][kubernetes admission webhooks]] to intercept api requests to the kubernetes api and then mutate responses so that the associated pv's bound uid is flushed upon deletion of a pvc.

 - Create a [[https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/ha-topology/][kubernetes highly-available topology]] to minimize application downtime and improve fault tolerance, and then utilize any of the previous steps to manage the failed pod restart.

The more suitable method would be to either use the admission webhooks or opt for a highly-available topology. Because this project is prioritizing learning different aspects of the cloud-native realms, I'm opting for a simpler manual approach for now.

#+begin_quote
*TL;DR*: Whenever the ~home-assistant~ pod or ~app-homeassistant~ pulumi stack is destroyed, we need to run the ~kubectl patch pv packages-volume --type json -p '[{"op": "remove", "path": "/spec/claimRef/uid"}]'~ command on the server before restarting the app stack/pod.
#+end_quote

*** TODO Implement either a webhook server or highly available topology to manage pvc reclaims
